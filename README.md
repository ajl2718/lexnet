# lexnet: learning neural nets from basics

Coding up a neural network library from scratch in order to learn the various aspects of building, training neural networks.

Currently:
- Feedforward networks with arbitrary numbers of layers and neurons in each layer
- Basic Stochastic Gradient
- Backpropagation (have yet to fully test that it is 100% working)
- Train and predict methods

Also some basic utilities:
- Functions to calculate accuracy, losses, one-hot encoding, relu, sigmoid and derivatives of these.

The ultimate goal is to be able to have a library that covers the types of neural networks that are taught in Christopher Manning's Deep Learning with NLP course:
http://web.stanford.edu/class/cs224n/
